# Ready to Test - nvCOMP 3.0.6 GPU Decode

## ‚úÖ Build Status: SUCCESS

The codec library has been successfully built with nvCOMP 3.0.6 integration.

## Run the Basic Test

On your RunPod instance, execute:

```bash
cd /workspace/CodecLLM
git pull
cd core
python test_gpu_direct_simple.py
```

Or use the convenience script:

```bash
cd /workspace/CodecLLM/core
bash RUN_BASIC_TEST.sh
```

## What This Test Does

1. **Creates test data**: 256x256 INT8 random array
2. **Compresses** using Zstd (CPU compression via libzstd)
3. **Decompresses to CPU** and verifies bit-exact reconstruction
4. **Decompresses to GPU** directly (nvCOMP 3.0.6) and verifies bit-exact reconstruction

## Expected Output

```
Testing GPU-direct decode...

‚úì GPU decoder available

Creating test data (256x256)...
  Data shape: (256, 256)
  Data size: 65536 bytes

Compressing...
  Compressed size: ~55000 bytes
  Ratio: ~1.18x

Decoding to CPU...
  ‚úì CPU decode successful
  Shape: (256, 256)
  ‚úì Bit-exact match

Decoding to GPU...
  ‚úì GPU decode successful
  ‚úì Data on GPU
  ‚úì Bit-exact match after copying back

All tests passed! ‚úì
```

## If It Works üéâ

You'll see:
- ‚úì GPU decoder available
- ‚úì CPU decode successful with bit-exact match
- ‚úì GPU decode successful with bit-exact match
- All tests passed!

This means:
1. nvCOMP 3.0.6 is working correctly
2. Compression/decompression pipeline is functional
3. GPU-direct decode is operational
4. Ready to test with real LLM!

## If It Fails ‚ùå

Copy the full output and let me know. We'll debug based on the error message.

Possible issues:
- `nvCOMP library not found` ‚Üí LD_LIBRARY_PATH issue
- `GPU decoder not available` ‚Üí nvCOMP not compiled in
- `Decode failed` ‚Üí API call issue
- `Mismatch` ‚Üí Data corruption bug

## Next Steps After Success

Once basic test passes, proceed to LLM inference test:

```bash
cd /workspace/CodecLLM/core
python test_zstd_inference.py
```

This will:
- Load TinyLlama-1.1B (2GB)
- Compress 20 Linear layers
- Run inference with GPU-direct decode
- Measure VRAM savings vs baseline

## Debug Commands (if needed)

Check library:
```bash
ldd /workspace/CodecLLM/core/build/libcodec_core.so | grep nvcomp
```

Check nvCOMP:
```bash
ls -la /usr/local/lib/libnvcomp*
python -c "import ctypes; print(ctypes.CDLL('/usr/local/lib/libnvcomp.so'))"
```

Check GPU:
```bash
nvidia-smi
```

---

**Ready to test!** Run the command and share the output. üöÄ

