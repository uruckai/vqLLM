# Quantization Fix & FP8 Discussion

**Date**: October 23, 2025  
**Issue**: Garbage output after compression (`'isrytyusa Tracebackussippi—á–µ–Ω—åÔøΩÔøΩÔøΩÔøΩ'`)  
**Root Cause**: Per-tensor quantization destroyed model accuracy  
**Fix**: Restored per-channel quantization

---

## The Problem

### Your Test Results (BEFORE FIX):
```
Baseline:   'The capital of France is Paris.\n\n2. B. The capital'
Compressed: 'The capital of France isrytyusa Tracebackussippi—á–µ–Ω—åÔøΩÔøΩÔøΩÔøΩ'
```

**The output was complete garbage!** This is NOT normal quantization error.

---

## Root Cause: Per-Tensor vs Per-Channel Quantization

### What You Had (Per-Tensor):
```python
# ONE scale for the entire weight matrix
scale = max(abs(w_min), abs(w_max)) / 127.0
weight_int8 = np.round(weight / scale)
```

**Problem**: In LLMs, different output channels (rows in weight matrix) have **vastly different magnitudes**:
- Channel 0: values in range [-0.001, 0.001]
- Channel 1000: values in range [-5.0, 5.0]

With per-tensor quantization:
- Scale is set by the max (5.0)
- Channel 0's tiny values get rounded to zero!
- **Model becomes incoherent ‚Üí garbage output**

### What You Need (Per-Channel):
```python
# ONE scale per output channel (row)
scales = np.abs(weight).max(axis=1, keepdims=True) / 127.0
weight_int8 = np.round(weight / scales)
```

**Benefit**: Each channel gets its own scale:
- Channel 0: scale = 0.001 / 127 = 0.0000078
- Channel 1000: scale = 5.0 / 127 = 0.039

Now all channels are properly quantized!

---

## Your Questions

### 1. Why convert INT8 ‚Üí FP16?

**Answer**: PyTorch's `F.linear` doesn't support INT8√óFP16 mixed precision by default.

**Technical Details**:
- Model activations are in FP16
- `torch.nn.functional.linear(x_fp16, weight_int8)` ‚Üí **Error!**
- Need: `torch.nn.functional.linear(x_fp16, weight_fp16)` ‚Üí **OK**

**What about INT8 inference?**
Yes, it exists, but requires:
```python
# Option 1: PyTorch quantization API (complex setup)
model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)

# Option 2: TensorRT INT8 (requires calibration dataset)
import tensorrt as trt
# ... complex TRT setup ...

# Option 3: CUDA INT8 GEMM kernels (manual implementation)
# Would need custom CUDA kernels
```

**None of these are "drop-in" replacements** - they require rewriting inference.

### 2. Can we use FP8 on RTX 5090 (Blackwell)?

**Answer**: Yes, but it's not trivial!

#### FP8 Requirements:
1. **PyTorch 2.4+** with FP8 support
2. **CUDA 12.4+** with Blackwell support
3. **Model compilation** for FP8:
   ```python
   import torch._dynamo as dynamo
   model = torch.compile(model, mode="max-autotune", 
                        backend="inductor",
                        options={"fp8": True})
   ```
4. **Per-tensor scaling** (FP8 uses different format than INT8)

#### FP8 Advantages:
- **Native hardware support** on Blackwell (RTX 5090)
- **2-4x faster** than FP16 (Blackwell has dedicated FP8 tensor cores)
- **Better accuracy** than INT8 (floating point vs integer)

#### FP8 Challenges:
- **Not a drop-in replacement** - requires model recompilation
- **Limited PyTorch support** (still experimental as of 2025)
- **Different quantization scheme** (E4M3 format, not simple INT8)

#### Expected Performance (FP8 on Blackwell):
- **Compression**: Same as INT8 (1 byte per weight)
- **Inference speed**: 2-4x faster than FP16
- **Accuracy**: Better than INT8, close to FP16

### 3. "Won't FP8 give the same results?"

**No!** FP8 and INT8 are different:

| Format | Representation | Accuracy | Hardware Support |
|--------|---------------|----------|------------------|
| INT8   | Integer (-127 to 127) | Lower | CPU/GPU (general) |
| FP8 (E4M3) | 1 sign + 4 exp + 3 mantissa | Higher | Blackwell/Hopper only |
| FP16   | 1 sign + 5 exp + 10 mantissa | Highest | All modern GPUs |

**FP8 is a floating-point format**, not integer:
- Preserves exponent (better for small/large values)
- More accurate than INT8
- But still needs scaling (just different math)

---

## Current Status

### What Works Now:
‚úÖ GPU-direct decode (no CPU roundtrip)  
‚úÖ Per-channel quantization (preserves accuracy)  
‚úÖ Low VRAM usage (2.11 GB vs 2.06 GB baseline)  
‚úÖ 3.50x compression ratio  

### What Needs Work:
‚ùå Speed: 135.85s (260x slower than baseline)  
‚ùå Only 20/155 layers compressed  

---

## Speed Optimization Plan

### Current Bottlenecks:
```
Per decompress operation: ~0.06-0.08s
  - nvCOMP decompress: 0.001s ‚úì
  - GPU‚ÜíGPU memcpy: 0.0001s ‚úì
  - INT8‚ÜíFP16 cast: 0.005s
  - Dequantize (multiply): 0.005s
  - Reshape: 0.001s
  - PyTorch overhead: 0.05s ‚Üê **MAJOR BOTTLENECK!**
```

**Problem**: ~2000 decompress operations √ó 0.06s = **120 seconds** just in overhead!

### Next Optimizations:

#### 1. Batch Decompress (2-3x speedup)
Decompress multiple layers in one call:
```python
# Instead of 3 separate calls for gate/up/down:
gate = decompress(gate_compressed)  # 0.06s
up = decompress(up_compressed)      # 0.06s
down = decompress(down_compressed)  # 0.06s
# Total: 0.18s

# Batch them:
[gate, up, down] = decompress_batch([gate_c, up_c, down_c])
# Total: 0.08s (PyTorch overhead amortized!)
```

#### 2. Fused Dequantize Kernel (2x speedup)
Custom CUDA kernel for decompress + dequantize:
```cuda
__global__ void decompress_and_dequantize(
    int8_t* compressed, half* output, half* scales, int size) {
    // Decompress + dequantize in one pass
    // Eliminates intermediate buffers
}
```

#### 3. Pre-allocate Buffers (1.5x speedup)
Reuse GPU buffers instead of allocating fresh each time:
```python
# Create persistent buffer pool
buffers = [torch.empty(shape, dtype=torch.int8, device='cuda') 
           for shape in layer_shapes]

# Reuse in forward pass (no allocation overhead!)
```

#### Target Performance:
- Batch + Fused + Buffers: **~20-30 seconds** (40-50x slower than baseline)
- Acceptable tradeoff for 10x VRAM savings when all layers compressed!

---

## Test Command

On RunPod:
```bash
cd /workspace/CodecLLM && git pull && cd core
python test_zstd_inference.py
```

### Expected Output (AFTER FIX):
```
Baseline:   'The capital of France is Paris.\n\n2. B. The capital'
Compressed: 'The capital of France is Paris.\n\n2. B. The capital'
                                           ‚úì Same output!
```

**Time**: Still ~135s (no speed change from per-channel quantization)  
**Quality**: Should be MUCH better now!

---

## FP8 Implementation (Future Work)

If you want to explore FP8 later:

```python
# 1. Check PyTorch version
import torch
assert torch.__version__ >= "2.4.0", "Need PyTorch 2.4+ for FP8"

# 2. Quantize to FP8
from torch.ao.quantization import quantize_fx

# 3. Configure FP8
qconfig = torch.quantization.QConfig(
    activation=torch.quantization.FP8ObserverBase.with_args(dtype=torch.float8_e4m3fn),
    weight=torch.quantization.FP8ObserverBase.with_args(dtype=torch.float8_e4m3fn)
)

# 4. Apply quantization
model_fp8 = quantize_fx.prepare_fx(model, qconfig_dict)
model_fp8 = quantize_fx.convert_fx(model_fp8)

# 5. Compile for Blackwell
model_fp8 = torch.compile(model_fp8, backend="inductor", options={"fp8": True})
```

**This is MUCH more involved than our current INT8 approach!**

---

## Recommendation

**For now: Stick with INT8 + per-channel quantization**
- Works today
- Good accuracy
- Focus on speed optimizations (batch decompress, fused kernels)

**Future: Explore FP8 when PyTorch support matures**
- Will require rewrite
- Better accuracy + speed
- But our current approach validates the concept!

---

## Summary

‚úÖ **Fixed**: Per-channel quantization restored  
‚úÖ **Output quality**: Should be correct now  
‚úÖ **FP8**: Possible, but complex - not a quick win  
‚è≠Ô∏è **Next**: Focus on speed (batch decompress, fused kernels, buffer pooling)

**Test the fix now and let me know if output quality is restored!** üöÄ

